{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CBOW.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rautshree/AI-class/blob/master/CBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7mpzd5VI0fd-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Resources\n",
        "A lot of resources are mentioned here: http://mccormickml.com/2016/04/27/word2vec-resources/\n",
        "\n",
        "As we know, CBOW is learning to predict the word by the context. Or maximize the probability of the target word by looking at the context. And this happens to be a problem for rare words. For example, given the context `yesterday was a really [...] day` CBOW model will tell you that most probably the word is `beautiful` or `nice`. Words like `delightful` will get much less attention of the model, because it is designed to predict the most probable word. This word will be smoothed over a lot of examples with more frequent words.\n",
        "\n",
        "On the other hand, the skip-gram model is designed to predict the context. Given the word `delightful` it must understand it and tell us that there is a huge probability that the context is `yesterday was really [...] day`, or some other relevant context. With skip-gram the word delightful will not try to compete with the word beautiful but instead, delightful+context pairs will be treated as new observations."
      ]
    },
    {
      "metadata": {
        "id": "N6mwXUgC0feD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import operator\n",
        "\n",
        "# Continuous Bag of Words model\n",
        "class CBOW(nn.Module):\n",
        "\n",
        "    def __init__(self, context_size=2, embedding_size=100, vocab_size=None):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.linear1 = nn.Linear(embedding_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        lookup_embeds = self.embeddings(inputs)\n",
        "        embeds = lookup_embeds.sum(dim=0)\n",
        "        out = self.linear1(embeds)\n",
        "        out = F.log_softmax(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    tensor = torch.LongTensor(idxs)\n",
        "    return autograd.Variable(tensor)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SpN3kA8Q5Rx4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bNJ_d5Ll13cy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5swUcoWs0feS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "EMBEDDING_SIZE = 10\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".lower().split()\n",
        "\n",
        "# How could you do a better pre-processing?\n",
        "# Maybe a sentence tokenizer?\n",
        "# Maybe a word lemmatizer?\n",
        "# Should you take a bigger corpus, Replace this small corpus with a bigger one\n",
        "# Maybe you should remove stopwords\n",
        "# Maybe you should just Google?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CvGbObu70fea",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca54d2be-b67b-4ded-9019-85535693fb5a"
      },
      "cell_type": "code",
      "source": [
        "# Create the  vocabulary\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "    context = [raw_text[i - 2], raw_text[i - 1],\n",
        "               raw_text[i + 1], raw_text[i + 2]]\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "\n",
        "print (data[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['we', 'are', 'to', 'study'], 'about')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7aOWigxn0feq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1791
        },
        "outputId": "2a69b979-b2e5-419c-b119-10f3b12c30a9"
      },
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "net = CBOW(CONTEXT_SIZE, embedding_size=EMBEDDING_SIZE, vocab_size=vocab_size)\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# The training loop\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "        context_var = make_context_vector(context, word_to_ix)\n",
        "        net.zero_grad()\n",
        "        ## Enter code to get log_probs from model\n",
        "        log_probs = net(context_var)\n",
        "        target = autograd.Variable(torch.LongTensor([word_to_ix[target]]))\n",
        "        loss = loss_func(log_probs.reshape(1,-1), target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.data\n",
        "    print(\"Loss for epoch \", epoch, \" : \", total_loss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss for epoch  0  :  tensor(251.2954)\n",
            "Loss for epoch  1  :  tensor(225.9308)\n",
            "Loss for epoch  2  :  tensor(206.3562)\n",
            "Loss for epoch  3  :  tensor(190.2513)\n",
            "Loss for epoch  4  :  tensor(176.5161)\n",
            "Loss for epoch  5  :  tensor(164.5693)\n",
            "Loss for epoch  6  :  tensor(154.0367)\n",
            "Loss for epoch  7  :  tensor(144.6463)\n",
            "Loss for epoch  8  :  tensor(136.1925)\n",
            "Loss for epoch  9  :  tensor(128.5194)\n",
            "Loss for epoch  10  :  tensor(121.5085)\n",
            "Loss for epoch  11  :  tensor(115.0685)\n",
            "Loss for epoch  12  :  tensor(109.1283)\n",
            "Loss for epoch  13  :  tensor(103.6313)\n",
            "Loss for epoch  14  :  tensor(98.5315)\n",
            "Loss for epoch  15  :  tensor(93.7913)\n",
            "Loss for epoch  16  :  tensor(89.3783)\n",
            "Loss for epoch  17  :  tensor(85.2647)\n",
            "Loss for epoch  18  :  tensor(81.4252)\n",
            "Loss for epoch  19  :  tensor(77.8370)\n",
            "Loss for epoch  20  :  tensor(74.4787)\n",
            "Loss for epoch  21  :  tensor(71.3309)\n",
            "Loss for epoch  22  :  tensor(68.3756)\n",
            "Loss for epoch  23  :  tensor(65.5966)\n",
            "Loss for epoch  24  :  tensor(62.9792)\n",
            "Loss for epoch  25  :  tensor(60.5106)\n",
            "Loss for epoch  26  :  tensor(58.1790)\n",
            "Loss for epoch  27  :  tensor(55.9742)\n",
            "Loss for epoch  28  :  tensor(53.8868)\n",
            "Loss for epoch  29  :  tensor(51.9083)\n",
            "Loss for epoch  30  :  tensor(50.0310)\n",
            "Loss for epoch  31  :  tensor(48.2479)\n",
            "Loss for epoch  32  :  tensor(46.5524)\n",
            "Loss for epoch  33  :  tensor(44.9383)\n",
            "Loss for epoch  34  :  tensor(43.4001)\n",
            "Loss for epoch  35  :  tensor(41.9326)\n",
            "Loss for epoch  36  :  tensor(40.5309)\n",
            "Loss for epoch  37  :  tensor(39.1907)\n",
            "Loss for epoch  38  :  tensor(37.9079)\n",
            "Loss for epoch  39  :  tensor(36.6787)\n",
            "Loss for epoch  40  :  tensor(35.4997)\n",
            "Loss for epoch  41  :  tensor(34.3679)\n",
            "Loss for epoch  42  :  tensor(33.2803)\n",
            "Loss for epoch  43  :  tensor(32.2344)\n",
            "Loss for epoch  44  :  tensor(31.2279)\n",
            "Loss for epoch  45  :  tensor(30.2587)\n",
            "Loss for epoch  46  :  tensor(29.3248)\n",
            "Loss for epoch  47  :  tensor(28.4245)\n",
            "Loss for epoch  48  :  tensor(27.5563)\n",
            "Loss for epoch  49  :  tensor(26.7188)\n",
            "Loss for epoch  50  :  tensor(25.9105)\n",
            "Loss for epoch  51  :  tensor(25.1305)\n",
            "Loss for epoch  52  :  tensor(24.3776)\n",
            "Loss for epoch  53  :  tensor(23.6509)\n",
            "Loss for epoch  54  :  tensor(22.9493)\n",
            "Loss for epoch  55  :  tensor(22.2722)\n",
            "Loss for epoch  56  :  tensor(21.6187)\n",
            "Loss for epoch  57  :  tensor(20.9880)\n",
            "Loss for epoch  58  :  tensor(20.3795)\n",
            "Loss for epoch  59  :  tensor(19.7924)\n",
            "Loss for epoch  60  :  tensor(19.2261)\n",
            "Loss for epoch  61  :  tensor(18.6800)\n",
            "Loss for epoch  62  :  tensor(18.1535)\n",
            "Loss for epoch  63  :  tensor(17.6458)\n",
            "Loss for epoch  64  :  tensor(17.1564)\n",
            "Loss for epoch  65  :  tensor(16.6847)\n",
            "Loss for epoch  66  :  tensor(16.2301)\n",
            "Loss for epoch  67  :  tensor(15.7920)\n",
            "Loss for epoch  68  :  tensor(15.3699)\n",
            "Loss for epoch  69  :  tensor(14.9632)\n",
            "Loss for epoch  70  :  tensor(14.5713)\n",
            "Loss for epoch  71  :  tensor(14.1937)\n",
            "Loss for epoch  72  :  tensor(13.8299)\n",
            "Loss for epoch  73  :  tensor(13.4794)\n",
            "Loss for epoch  74  :  tensor(13.1416)\n",
            "Loss for epoch  75  :  tensor(12.8160)\n",
            "Loss for epoch  76  :  tensor(12.5023)\n",
            "Loss for epoch  77  :  tensor(12.1998)\n",
            "Loss for epoch  78  :  tensor(11.9083)\n",
            "Loss for epoch  79  :  tensor(11.6272)\n",
            "Loss for epoch  80  :  tensor(11.3561)\n",
            "Loss for epoch  81  :  tensor(11.0947)\n",
            "Loss for epoch  82  :  tensor(10.8425)\n",
            "Loss for epoch  83  :  tensor(10.5991)\n",
            "Loss for epoch  84  :  tensor(10.3643)\n",
            "Loss for epoch  85  :  tensor(10.1375)\n",
            "Loss for epoch  86  :  tensor(9.9186)\n",
            "Loss for epoch  87  :  tensor(9.7072)\n",
            "Loss for epoch  88  :  tensor(9.5029)\n",
            "Loss for epoch  89  :  tensor(9.3055)\n",
            "Loss for epoch  90  :  tensor(9.1148)\n",
            "Loss for epoch  91  :  tensor(8.9303)\n",
            "Loss for epoch  92  :  tensor(8.7519)\n",
            "Loss for epoch  93  :  tensor(8.5793)\n",
            "Loss for epoch  94  :  tensor(8.4123)\n",
            "Loss for epoch  95  :  tensor(8.2506)\n",
            "Loss for epoch  96  :  tensor(8.0941)\n",
            "Loss for epoch  97  :  tensor(7.9425)\n",
            "Loss for epoch  98  :  tensor(7.7956)\n",
            "Loss for epoch  99  :  tensor(7.6532)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oPyVvH330ffA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now let's find embedding for every word\n",
        "vocab_to_embedding = {}\n",
        "for word in vocab:\n",
        "    vocab_to_embedding[word] = net.embeddings.forward(make_context_vector([word], word_to_ix))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5J2c79MB0ffI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_k_similar_words(word, k = 5):\n",
        "    word = word.lower()\n",
        "    if word not in vocab:\n",
        "        print (\"Not found \", word)\n",
        "        return []\n",
        "    a = vocab_to_embedding[word]\n",
        "    max_sim = -1\n",
        "    sim_here = {}\n",
        "    for b in vocab_to_embedding:\n",
        "        emb = vocab_to_embedding[b]\n",
        "        sim = torch.dot(a.reshape(-1),emb.reshape(-1))/(a.norm()*emb.norm())\n",
        "        sim_here[b] = sim.data[0]\n",
        "    sorted_t = sorted(sim_here.items(), key=operator.itemgetter(1))\n",
        "    sorted_t.reverse()\n",
        "    return sorted_t[:k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YA36TOCY0ffS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "75aa284c-63fb-4e19-c80c-7e2944cf274e"
      },
      "cell_type": "code",
      "source": [
        "find_k_similar_words('program.', 5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('program.', tensor(1.)),\n",
              " ('by', tensor(0.4817)),\n",
              " ('process.', tensor(0.4519)),\n",
              " ('idea', tensor(0.4416)),\n",
              " ('process', tensor(0.3935))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "ZX9GJwj10fff",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Could you define a Skip Gram model?"
      ]
    },
    {
      "metadata": {
        "id": "nP4qASiC0ffm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}