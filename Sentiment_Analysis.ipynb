{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rautshree/AI-class/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZctkwS4dmwwB",
        "colab_type": "code",
        "outputId": "b18447ec-b035-4d06-c730-21a7b7f11c33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, Convolution1D, Flatten, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "50q6Sx0orli-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this analysis we’ll be using a dataset of 50,000 movie reviews taken from IMDb. The data was compiled by Andrew Maas and can be found here: [IMDb Reviews.](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "\n",
        "The data is split evenly with 25k reviews intended for training and 25k for testing your classifier. Moreover, each set has 12.5k positive and 12.5k negative reviews.\n",
        "\n",
        "IMDb lets users rate movies on a scale from 1 to 10. To label these reviews the curator of the data labeled anything with ≤ 4 stars as negative and anything with ≥ 7 stars as positive. Reviews with 5 or 6 stars were left out.\n",
        "\n",
        "Ofcourse, we can download it and do the pre-processing ourselves, well well well Keras already provides a wrapper over pre-processed dataset."
      ]
    },
    {
      "metadata": {
        "id": "7f1URxjBm8TE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f595e109-a0a5-46e7-ef5e-d45484fd1d60"
      },
      "cell_type": "code",
      "source": [
        "top_words = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eJ1eayclnCyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3a3fc6af-f254-4ebb-a93b-5f816bf845b2"
      },
      "cell_type": "code",
      "source": [
        "# These words are encoded into numbers by the index of their place in the vocabulary.\n",
        "print (X_train[0], y_train[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-SkdDd12ywNz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3716
        },
        "outputId": "920411a8-5b73-476f-b440-ac535b6a3794"
      },
      "cell_type": "code",
      "source": [
        "# Let us decode a sentence to see what it holds\n",
        "d = imdb.get_word_index()\n",
        "inv_map = {v: k for k, v in d.items()}\n",
        "for i in X_train[0][1:]:\n",
        "  if (i>=3):\n",
        "    print (inv_map[i-3])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "this\n",
            "film\n",
            "was\n",
            "just\n",
            "brilliant\n",
            "casting\n",
            "location\n",
            "scenery\n",
            "story\n",
            "direction\n",
            "everyone's\n",
            "really\n",
            "suited\n",
            "the\n",
            "part\n",
            "they\n",
            "played\n",
            "and\n",
            "you\n",
            "could\n",
            "just\n",
            "imagine\n",
            "being\n",
            "there\n",
            "robert\n",
            "is\n",
            "an\n",
            "amazing\n",
            "actor\n",
            "and\n",
            "now\n",
            "the\n",
            "same\n",
            "being\n",
            "director\n",
            "father\n",
            "came\n",
            "from\n",
            "the\n",
            "same\n",
            "scottish\n",
            "island\n",
            "as\n",
            "myself\n",
            "so\n",
            "i\n",
            "loved\n",
            "the\n",
            "fact\n",
            "there\n",
            "was\n",
            "a\n",
            "real\n",
            "connection\n",
            "with\n",
            "this\n",
            "film\n",
            "the\n",
            "witty\n",
            "remarks\n",
            "throughout\n",
            "the\n",
            "film\n",
            "were\n",
            "great\n",
            "it\n",
            "was\n",
            "just\n",
            "brilliant\n",
            "so\n",
            "much\n",
            "that\n",
            "i\n",
            "bought\n",
            "the\n",
            "film\n",
            "as\n",
            "soon\n",
            "as\n",
            "it\n",
            "was\n",
            "released\n",
            "for\n",
            "and\n",
            "would\n",
            "recommend\n",
            "it\n",
            "to\n",
            "everyone\n",
            "to\n",
            "watch\n",
            "and\n",
            "the\n",
            "fly\n",
            "fishing\n",
            "was\n",
            "amazing\n",
            "really\n",
            "cried\n",
            "at\n",
            "the\n",
            "end\n",
            "it\n",
            "was\n",
            "so\n",
            "sad\n",
            "and\n",
            "you\n",
            "know\n",
            "what\n",
            "they\n",
            "say\n",
            "if\n",
            "you\n",
            "cry\n",
            "at\n",
            "a\n",
            "film\n",
            "it\n",
            "must\n",
            "have\n",
            "been\n",
            "good\n",
            "and\n",
            "this\n",
            "definitely\n",
            "was\n",
            "also\n",
            "to\n",
            "the\n",
            "two\n",
            "little\n",
            "boy's\n",
            "that\n",
            "played\n",
            "the\n",
            "of\n",
            "norman\n",
            "and\n",
            "paul\n",
            "they\n",
            "were\n",
            "just\n",
            "brilliant\n",
            "children\n",
            "are\n",
            "often\n",
            "left\n",
            "out\n",
            "of\n",
            "the\n",
            "list\n",
            "i\n",
            "think\n",
            "because\n",
            "the\n",
            "stars\n",
            "that\n",
            "play\n",
            "them\n",
            "all\n",
            "grown\n",
            "up\n",
            "are\n",
            "such\n",
            "a\n",
            "big\n",
            "profile\n",
            "for\n",
            "the\n",
            "whole\n",
            "film\n",
            "but\n",
            "these\n",
            "children\n",
            "are\n",
            "amazing\n",
            "and\n",
            "should\n",
            "be\n",
            "praised\n",
            "for\n",
            "what\n",
            "they\n",
            "have\n",
            "done\n",
            "don't\n",
            "you\n",
            "think\n",
            "the\n",
            "whole\n",
            "story\n",
            "was\n",
            "so\n",
            "lovely\n",
            "because\n",
            "it\n",
            "was\n",
            "true\n",
            "and\n",
            "was\n",
            "someone's\n",
            "life\n",
            "after\n",
            "all\n",
            "that\n",
            "was\n",
            "shared\n",
            "with\n",
            "us\n",
            "all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "llqmZra5nKIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_review_length = 80 # ? You need to change this, maybe 100, maybe 1000, how do you find it? Maybe the max length of reviews or maybe something for faster training! Do you always need to compromise in life?\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)#?? # What if we don't pad the test data? "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VrUzgUvXnOEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "3a958ba0-59c0-40ed-d637-bd5abe1e08a4"
      },
      "cell_type": "code",
      "source": [
        "print (X_train[0], len(X_train[0]))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16 5345   19  178   32] 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4QcA32Sns9r9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Conv1D](https://cdn-images-1.medium.com/max/1600/1*aBN2Ir7y2E-t2AbekOtEIw.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "D6yffxb2nQ9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "2bc63864-8c05-4a58-dc2b-af4cc67d26f2"
      },
      "cell_type": "code",
      "source": [
        "embedding_vector_length = 300\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
        "# Some bad architecture choice here, Could you please try to find what's wrong?\n",
        "model.add(Convolution1D(64, 3, padding='same')) # Why padding is same? https://keras.io/layers/convolutional/\n",
        "model.add(Convolution1D(32, 3, padding='same'))\n",
        "model.add(Convolution1D(16, 3, padding='same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2)) # Should you increase this for more generalization?\n",
        "model.add(Dense(180,activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1,activation='sigmoid')) # Can we use 2 here?\n",
        "print (model.summary())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 80, 300)           3000000   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 80, 64)            57664     \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 80, 32)            6176      \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 80, 16)            1552      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 180)               230580    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 180)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 181       \n",
            "=================================================================\n",
            "Total params: 3,296,153\n",
            "Trainable params: 3,296,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5_7xVu1WnTI0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x8G-0F1nnY3u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "6d00f112-f7dc-4698-9863-f1f2e9563cf5"
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, nb_epoch=2, verbose = 1, batch_size = 64)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "25000/25000 [==============================] - 54s 2ms/step - loss: 0.4438 - acc: 0.7797\n",
            "Epoch 2/2\n",
            "25000/25000 [==============================] - 52s 2ms/step - loss: 0.2386 - acc: 0.9039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff42e818358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "yM4xeesBnm_X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ae0511f8-996f-42d7-a1db-e58da59d283b"
      },
      "cell_type": "code",
      "source": [
        "loss_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % \\\n",
        "  (loss_acc[0], loss_acc[1]*100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 12s 491us/step\n",
            "Test data: loss = 0.418050  accuracy = 82.32% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c2wjuPSNt_7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "680b1be2-3e1e-4c2b-9a16-61389321b30e"
      },
      "cell_type": "code",
      "source": [
        "# 5. save model\n",
        "print(\"Saving model to disk \\n\")\n",
        "mp = \".\\\\Models\\\\imdb_model.h5\"\n",
        "model.save(mp)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to disk \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eEWW_hN1uLFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3854238a-fca3-4902-c74d-137ad03befbf"
      },
      "cell_type": "code",
      "source": [
        "print(\"New review: \\'the school was good\\'\")\n",
        "review = \"the school was good\"\n",
        "words = review.split()\n",
        "review = []\n",
        "for word in words:\n",
        "  if word not in d: \n",
        "    review.append(2)\n",
        "  else:\n",
        "    review.append(d[word]+3) "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New review: 'the school was good'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HGbLP_50biWb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "review = [0]*(max_review_length-len(review)) + review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s7-amy-kuY7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b953b97c-4b35-43a7-b22a-6343d5b034c4"
      },
      "cell_type": "code",
      "source": [
        "# review = sequence.pad_sequences(review, maxlen=80)\n",
        "import numpy as np\n",
        "review = np.array(review)\n",
        "print (review.shape)\n",
        "prediction = model.predict(review.reshape(1,80))\n",
        "print(\"Prediction (0 = negative, 1 = positive) = \", end=\"\")\n",
        "print(\"%0.4f\" % prediction[0][0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80,)\n",
            "Prediction (0 = negative, 1 = positive) = 0.3972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w6jw8WWVzvSe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's  try an LSTM based model, **could you change this to BiLSTM model?**\n",
        "\n",
        "LSTMs and their bidirectional variants are popular because they have tried to learn how and when to forget and when not to using gates in their architecture. In previous RNN architectures, vanishing gradients was a big problem and caused those nets not to learn so much.\n",
        "\n",
        "Using Bidirectional LSTMs, you feed the learning algorithm with the original data once from beginning to the end and once from end to beginning."
      ]
    },
    {
      "metadata": {
        "id": "9rh8ucBUufkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "5713c560-4ddc-4f25-a184-ddab06c3ea3f"
      },
      "cell_type": "code",
      "source": [
        "import keras as K\n",
        "model = K.models.Sequential()\n",
        "model.add(K.layers.embeddings.Embedding(input_dim=top_words,\n",
        "  output_dim=embedding_vector_length, mask_zero=True))\n",
        "model.add(K.layers.LSTM(units=100, dropout=0.2, recurrent_dropout=0.2))  # 100 memory\n",
        "model.add(K.layers.Dense(units=1, activation='sigmoid'))\n",
        "print (model.summary())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 300)         3000000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 3,160,501\n",
            "Trainable params: 3,160,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e6jae4Dac-tx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjv7ZfZWdBdV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "764dcecf-4835-4392-8ea2-337d00cf74c7"
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, nb_epoch=2, verbose = 1, batch_size = 64)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "25000/25000 [==============================] - 152s 6ms/step - loss: 0.4576 - acc: 0.7844\n",
            "Epoch 2/2\n",
            "25000/25000 [==============================] - 151s 6ms/step - loss: 0.3182 - acc: 0.8689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff40f092470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "r6lJgPrzdDwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "539a3210-a6c7-4a1e-919a-ba8ef1ceb4b0"
      },
      "cell_type": "code",
      "source": [
        "loss_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % \\\n",
        "  (loss_acc[0], loss_acc[1]*100))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 53s 2ms/step\n",
            "Test data: loss = 0.376176  accuracy = 84.05% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sfmvdBuDeUEA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}